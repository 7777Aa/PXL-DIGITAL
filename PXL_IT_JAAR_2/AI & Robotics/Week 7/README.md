# Week 7: Supervised Learning: Decision Trees

## 1. Machine Learning
### Goals
The junior-colleague
* can explain Machine Learning  in their own words
* can situate Machine Learning in the broader context of AI
* can explain the difference between traditional programming and Machine Learning
* can explain the importance of practical Machine Learning
* can describe a standard ML pipeline

**[Presentation](Week%207%20-%201%20Machine%20Learning%20Intro.pdf)**

## 2. Supervised Learning
### Goals
The junior-colleague
* can explain Supervised Learning in their own words
* can describe the general flow of a supervised learning pipeline
* can explain the difference between classification and regression
* can explain the difference between linear and nonlinear spread of classification examples
* can explain the difference between structured and unstructured data

**[Presentation](Week%207%20-%202%20Supervised%20Learning.pdf)**


### Extra
**[Data Advanced: Machine Learning](extra%20material)**

## 3. Decision Trees
### Goals
The junior-colleague
* can explain in their own words what a decision tree is
* can explain the difference between classification and regression in context of a decision tree
* can explain the usages of decision trees 
* can explain how to decide on a split point for a classification tree based on the gini index
* can explain how to decide on a split point for a classification tree based on entropy and information gain
* can explain how to decide on a split point for a regression tree
* can construct a classification decision tree for a given problem based on the gini index
* can construct a classification decision tree for a given problem based on entropy and information gain
* can implement a decision tree for a given problem in scikit-learn
* can explain the advantages and disadvantages of decision trees

**[Presentation](Week%207%20-%203%20Decision%20Trees.pdf)**

### Extra
- Youtube video on Decision Trees: https://www.youtube.com/watch?time_continue=2&v=LDRbO9a6XPU
(Thanks, Wesley!)

## 4. How good is my model?
### Goals
The junior-colleague
* can explain in their own words the difficulty of describing the “goodness” of a ML model
* can explain in their own words the metric used for classification
* can explain in their own words the metrics used for regression
* can describe how to go about measuring the “goodness” of an ML model
* can explain overfitting and underfitting in their own words
* can explain  the tradeoff between bias and variance
* can explain the importance of dividing your data sets in training, validation and test sets
* can explain the purpose of the training set
* can explain the purpose of the validation set
* can explain the purpose of the test set

**[Presentation](Week%207%20-%204%20How%20good%20is%20my%20model_.pdf)**


### Extra
**MSE**
- https://medium.freecodecamp.org/machine-learning-mean-squared-error-regression-line-c7dde9a26b93

**R²** 
- https://en.wikipedia.org/wiki/Coefficient_of_determination
- https://view.officeapps.live.com/op/view.aspx?src=http://www2.gsu.edu/~dscaas/pptdsc/regression.ppt


## 5. Random Forests
### Goals
The junior-colleague
* can explain bagging in the context of random forests in their own words
* can explain bootstrap sampling
* can explain a random forest in their own words
* can explain the importance of random sampling in the context of random forests
* can describe the habit of overfitting in context of decision trees
* can describe why the different decision trees in a random forest need to be as uncorrelated as possible
* is able to sum up and explain 5 advantages and 2 disadvantages of random forests

**[Presentation](Week%207%20-%205%20Random%20Forests.pdf)**

## Exercises
The PDF file with all the exercises can be found **[here](exercises/Week%207.pdf)**.
