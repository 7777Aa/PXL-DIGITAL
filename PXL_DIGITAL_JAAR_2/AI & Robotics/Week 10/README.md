# Week 10: Boosting

## 1. Boosting
### Goals
The junior-colleague
* can explain boosting in their own words
* can describe the stopping criteria for boosting algorithms
* can explain AdaBoost in their own words
* can explain what a loss function is and why we use them
* can explain the idea behind gradient descent in their own words
* can explain the importance of the learning rate in the context of gradient descent
* can explain gradient boosting in their own words
* can explain the differences between bagging and boosting
* can explain the advantages of XGBoost over other bagging and boosting algorithms


**[Presentation](Week%2010%20-%201%20Boosting.pdf)**

### Extra
- https://hackernoon.com/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c
- https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/
- https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d
- https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html
- https://explained.ai/gradient-boosting/
- https://www.datacamp.com/community/tutorials/xgboost-in-python=

## Exercises
The PDF file with all the exercises can be found **[here](exercises/Week%2010.pdf)**.
